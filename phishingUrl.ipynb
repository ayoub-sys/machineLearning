{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN98R68qI4xMIxqfdr3mFc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayoub-sys/machineLearning/blob/main/phishingUrl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHISHING URL DETECTION"
      ],
      "metadata": {
        "id": "teCDTOxER-bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipaddress\n",
        "import re\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import socket\n",
        "import requests\n",
        "from googlesearch import search\n",
        "import whois\n",
        "from datetime import datetime\n",
        "import time\n",
        "from dateutil.parser import parse as date_parse\n",
        "\n",
        "# Calculates number of months\n",
        "def diff_month(d1, d2):\n",
        "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
        "\n",
        "# Generate data set by extracting the features from the URL\n",
        "def generate_data_set(url):\n",
        "\n",
        "    data_set = []\n",
        "\n",
        "    # Converts the given URL into standard format\n",
        "    if not re.match(r\"^https?\", url):\n",
        "        url = \"http://\" + url\n",
        "\n",
        "\n",
        "    # Stores the response of the given URL\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    except:\n",
        "        response = \"\"\n",
        "        soup = -999\n",
        "\n",
        "\n",
        "    # Extracts domain from the given URL\n",
        "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
        "    if re.match(r\"^www.\",domain):\n",
        "\t       domain = domain.replace(\"www.\",\"\")\n",
        "\n",
        "    # Requests all the information about the domain\n",
        "    whois_response = whois.whois(domain)\n",
        "\n",
        "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
        "        \"name\": domain\n",
        "    })\n",
        "\n",
        "    # Extracts global rank of the website\n",
        "    try:\n",
        "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
        "    except:\n",
        "        global_rank = -1\n",
        "\n",
        "    # 1.having_IP_Address\n",
        "    try:\n",
        "        ipaddress.ip_address(url)\n",
        "        data_set.append(-1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 2.URL_Length\n",
        "    if len(url) < 54:\n",
        "        data_set.append(1)\n",
        "    elif len(url) >= 54 and len(url) <= 75:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 3.Shortining_Service\n",
        "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
        "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
        "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
        "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
        "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
        "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
        "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
        "    if match:\n",
        "        data_set.append(match)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 4.having_At_Symbol\n",
        "    if re.findall(\"@\", url):\n",
        "        data_set.append(url)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 5.double_slash_redirecting\n",
        "    list=[x.start(0) for x in re.finditer('//', url)]\n",
        "    if list[len(list)-1]>6:\n",
        "        data_set.append(list)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 6.Prefix_Suffix\n",
        "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
        "        data_set.append(url)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 7.having_Sub_Domain\n",
        "    if len(re.findall(\"\\.\", url)) == 1:\n",
        "        data_set.append(1)\n",
        "    elif len(re.findall(\"\\.\", url)) == 2:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 8.SSLfinal_State\n",
        "    try:\n",
        "        if response.text:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 9.Domain_registeration_length\n",
        "    expiration_date = whois_response.expiration_date\n",
        "    registration_length = 0\n",
        "    try:\n",
        "        expiration_date = min(expiration_date)\n",
        "        today = time.strftime('%Y-%m-%d')\n",
        "        today = datetime.strptime(today, '%Y-%m-%d')\n",
        "        registration_length = abs((expiration_date - today).days)\n",
        "\n",
        "        if registration_length / 365 <= 1:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 10.Favicon\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        try:\n",
        "            for head in soup.find_all('head'):\n",
        "                for head.link in soup.find_all('link', href=True):\n",
        "                    dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
        "                    if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
        "                        data_set.append(1)\n",
        "                        raise StopIteration\n",
        "                    else:\n",
        "                        data_set.append(-1)\n",
        "                        raise StopIteration\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    #11. port\n",
        "    try:\n",
        "        port = domain.split(\":\")[1]\n",
        "        if port:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    #12. HTTPS_token\n",
        "    if re.findall(r\"^https://\", url):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #13. Request_URL\n",
        "    i = 0\n",
        "    success = 0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for img in soup.find_all('img', src= True):\n",
        "           dots= [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
        "           if url in img['src'] or domain in img['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for audio in soup.find_all('audio', src= True):\n",
        "           dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
        "           if url in audio['src'] or domain in audio['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for embed in soup.find_all('embed', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',embed['src'])]\n",
        "           if url in embed['src'] or domain in embed['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for iframe in soup.find_all('iframe', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',iframe['src'])]\n",
        "           if url in iframe['src'] or domain in iframe['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        try:\n",
        "           percentage = success/float(i) * 100\n",
        "           if percentage < 22.0 :\n",
        "              data_set.append(1)\n",
        "           elif((percentage >= 22.0) and (percentage < 61.0)) :\n",
        "              data_set.append(0)\n",
        "           else :\n",
        "              data_set.append(-1)\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "\n",
        "\n",
        "    #14. URL_of_Anchor\n",
        "    percentage = 0\n",
        "    i = 0\n",
        "    unsafe=0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for a in soup.find_all('a', href=True):\n",
        "        # 2nd condition was 'JavaScript ::void(0)' but we put JavaScript because the space between javascript and :: might not be\n",
        "                # there in the actual a['href']\n",
        "            if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or domain in a['href']):\n",
        "                unsafe = unsafe + 1\n",
        "            i = i + 1\n",
        "\n",
        "\n",
        "        try:\n",
        "            percentage = unsafe / float(i) * 100\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "        if percentage < 31.0:\n",
        "            data_set.append(1)\n",
        "        elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #15. Links_in_tags\n",
        "    i=0\n",
        "    success =0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for link in soup.find_all('link', href= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',link['href'])]\n",
        "           if url in link['href'] or domain in link['href'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for script in soup.find_all('script', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',script['src'])]\n",
        "           if url in script['src'] or domain in script['src'] or len(dots)==1 :\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "        try:\n",
        "            percentage = success / float(i) * 100\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "        if percentage < 17.0 :\n",
        "           data_set.append(1)\n",
        "        elif((percentage >= 17.0) and (percentage < 81.0)) :\n",
        "           data_set.append(0)\n",
        "        else :\n",
        "           data_set.append(-1)\n",
        "\n",
        "        #16. SFH\n",
        "        for form in soup.find_all('form', action= True):\n",
        "           if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
        "              data_set.append(-1)\n",
        "              break\n",
        "           elif url not in form['action'] and domain not in form['action']:\n",
        "               data_set.append(0)\n",
        "               break\n",
        "           else:\n",
        "                 data_set.append(1)\n",
        "                 break\n",
        "\n",
        "    #17. Submitting_to_email\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #18. Abnormal_URL\n",
        "    if response == \"\":\n",
        "        data_set.append(response)\n",
        "    else:\n",
        "        if response.text == \"\":\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(response.text)\n",
        "\n",
        "    #19. Redirect\n",
        "    if response == \"\":\n",
        "        data_set.append(response)\n",
        "    else:\n",
        "        if len(response.history) <= 1:\n",
        "            data_set.append(-1)\n",
        "        elif len(response.history) <= 4:\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #20. on_mouseover\n",
        "    if response == \"\" :\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #21. RightClick\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #22. popUpWidnow\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"alert\\(\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #23. Iframe\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #24. age_of_domain\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        try:\n",
        "            registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
        "            if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
        "                data_set.append(-1)\n",
        "            else:\n",
        "                data_set.append(1)\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #25. DNSRecord\n",
        "    dns = 1\n",
        "    try:\n",
        "        d = whois.whois(domain)\n",
        "    except:\n",
        "        dns=-1\n",
        "    if dns == -1:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if registration_length / 365 <= 1:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #26. web_traffic\n",
        "    try:\n",
        "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
        "        rank= int(rank)\n",
        "        if (rank<100000):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(0)\n",
        "    except TypeError:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #27. Page_Rank\n",
        "    try:\n",
        "        if global_rank > 0 and global_rank < 100000:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    #28. Google_Index\n",
        "    site=search(url, 5)\n",
        "    if site:\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #29. Links_pointing_to_page\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
        "        if number_of_links == 0:\n",
        "            data_set.append(1)\n",
        "        elif number_of_links <= 2:\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #30. Statistical_report\n",
        "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
        "    try:\n",
        "        ip_address=socket.gethostbyname(domain)\n",
        "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
        "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
        "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
        "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
        "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
        "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
        "        if url_match:\n",
        "            data_set.append(-1)\n",
        "        elif ip_match:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        print ('Connection problem. Please check your internet connection!')\n",
        "\n",
        "\n",
        "    print (data_set)\n",
        "    return data_set"
      ],
      "metadata": {
        "id": "W6CLE83dSF1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spcACRFMnLoa",
        "outputId": "acbe3f99-b2a5-495c-91cc-fc2663c8ec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-whois\n",
            "  Downloading python-whois-0.8.0.tar.gz (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from python-whois) (0.16.0)\n",
            "Building wheels for collected packages: python-whois\n",
            "  Building wheel for python-whois (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-whois: filename=python_whois-0.8.0-py3-none-any.whl size=103263 sha256=b44eee8b3d60da91a1441b9b889c07da83cf4228eca3d4db030855366bcf8c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/7d/a1/7cfba90ff34974c64149b70f689ff77dde232b8f1ec5de43b3\n",
            "Successfully built python-whois\n",
            "Installing collected packages: python-whois\n",
            "Successfully installed python-whois-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txq0xdM0TnT0",
        "outputId": "796d8a53-2038-4d02-c9fe-f76be3e2643f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from python-whois) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#import feature_extraction\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression as lr\n",
        "from flask import jsonify\n",
        "\n",
        "from google.colab import files \n",
        "uploaded=files.upload()\n",
        "def getResult(url):\n",
        "\n",
        "    #Importing dataset\n",
        "    data = np.loadtxt(\"phishing_data.csv\", delimiter = \",\")\n",
        "\n",
        "    #Seperating features and labels\n",
        "    X = data[: , :-1]\n",
        "    y = data[: , -1]\n",
        "\n",
        "    #Seperating training features, testing features, training labels & testing labels\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "    clf = rfc()\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(score*100)\n",
        "    X_new = []\n",
        "\n",
        "    X_input = url\n",
        "    X_new=generate_data_set(X_input)\n",
        "    X_new = np.array(X_new).reshape(1,-1)\n",
        "\n",
        "    try:\n",
        "        prediction = clf.predict(X_new)\n",
        "        if prediction == -1:\n",
        "            return \"Phishing Url\"\n",
        "        else:\n",
        "            return \"Legitimate Url\"\n",
        "    except:\n",
        "        return \"Phishing Url\"\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "6xqOe1xtV34U",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "cd8c84cc-a31a-4e52-dd67-610e9176fded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8872c0d6-8065-413f-8f9f-5cfc6fa2a422\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8872c0d6-8065-413f-8f9f-5cfc6fa2a422\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving phishing_data.csv to phishing_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getResult('https://www.google.com')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "J_u6TIj7W7U_",
        "outputId": "a4ef5545-3fc9-4a20-c725-dadcdcb6fc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1aa6afc35c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.google.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'getResult' is not defined"
          ]
        }
      ]
    }
  ]
}